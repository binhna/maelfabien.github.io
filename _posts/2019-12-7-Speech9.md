---
published: true
title: Sound Feature Extraction
collection: ml
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Signal Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

Sound features can be used to detect speakers, detect the gender, the age, diseases and much more through the voice.

To extract features, we must break down the audio file into windows, often between 20 and 100 milliseconds. We then extract these features per window and can run a classification algorithm for example on each window.

# 1. Statistical Features

A first easy step is to compute the mean, standard deviation, minimum, maximum, median and quartiles of the amplitude of each window. This can be done using Numpy and it always brings value to our feature extraction.

```python
mean=np.mean(matrix)
std=np.std(matrix) 
maxv=np.amax(matrix) 
minv=np.amin(matrix) 
median=np.median(matrix)

output=np.array([mean,std,maxv,minv,median])
```

# 2. Mel Frequency Cepstral Coefficients (MFCC)

My understanding of MFCC highly relies on [this excellent article](http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/).

The MFCC are state-of-the-art features for speaker identification, disease detection, speech recognition...

Start by taking a short window frame (20 to 40 ms) in which we can assume that the audio signal is stationary. We then select a frame step (e.g. rolling window) of around 10 ms.

We then compute the power spectrum of each frame through a periodogram, which is inspired by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. To do so, start by taking the Discrete Fourrier Transform of the frame:

$$ S_i(k) = \sum_{n=1}^N s_i(n)h(n) e^{-j 2 \pi k n / N} $$

where:
- $$ s_i(n) $$ is the framed time signal (i frames)
- $$ N $$ is the number of samples in a Hamming Window
- $$ h(n) $$ is the Hamming Window
- $$ K $$ is the length of the DFT

To compute the periodogram estimate of the power spectrum, we apply:

$$ P_i(k) = \frac{1}{N} {\mid S_i(k) \mid}^2

Since the cochlea is not so good to discriminate between two closely spaced frequencies, especially when they are high, we take clumps of periodogram bins and sum them up. This is done by applying Mel filterbank, filters which tell us exactly how to space our filterbanks and how wide to make them. These filters are typically narrower around 0Hz and wider for higher frequencies.

![image](https://maelfabien.github.io/assets/images/melfb.png)

The formula to move from frequencies to Mel scale is the following:

$$ M(f) = 1125 ln (1 + \frac{f}{700}) $$

The Mel filterbank is a set of 26 triangular filters which we apply to the periodogram power spectral estimate. Each filter is mostly made of 0's but has a non-zero triangle in some region. We multiply the values of the periodogram by the ones of the filters.

![image](https://maelfabien.github.io/assets/images/melfb2.png)

We then take the logarithm of the all those 26 series of energy of those filterbanks since we do not percieve loudness linearly, but close to logarithmically.

We finally apply a Discrete Cosine Transform to the 26 log filterbank energies in order to decorrelate the overlapping filterbanks energies. This gives us 26 coefficients, called the MFCC. Not all of them are useful, and for Automatic Speech Recognition, we typically only use the 12-13 lower values.

MFCCs are widely used to classify phonemes. They can easily be extracted using `librosa` library:

```python
import librosa
y, sr = librosa.load(filename)
mfcc=librosa.feature.mfcc(y)
```

It returns a numpy array of size 20 (MFCC extracted) * the number of windows (for the file `test.wav`, 431).

```python
array([[  22.456623 ,  -19.06088  , -164.62514  , ..., -240.06525  ,
        -257.81137  , -260.06912  ],
       [  95.18431  ,   89.34006  ,   37.92323  , ...,  171.81778  ,
         160.94362  ,   97.23265  ],
...
```

Since these series can get quite long as one new data point is created every 20ms, one can always extract the mean, variance, quartiles, min, max and median as a descriptive statistic at the end of an audio sample, and compare several audio samples on this basis.

# 3. Mel Frequency Cepstral Differential Coefficients

MFCC lacks information on the evolution of the coefficients between frames. What we can therefore do is to compute the 12 trajectories of the MFC coefficients and append them to the 12 original coefficients. This highly improves results on ASR tasks.

$$ d_t = \frac{\sum_{n=1}^N n (c_{t+n} - c_{t-n})}{2 \sum_{n=1}^N n^2} $$

Where:
- $$ d_t $$ is the delta coefficient
- $$ t $$ is the frame considered
- $$ c_t $$ is the coefficient at time $$ t $$ (we commonly use N=2)

# 4. Fundamental Frequency

One can also extract fundamental frequencies of a voice, which are the lowest frequencies of a periodic voice waveform. This is really useful for classifying gender, since males have lower fundamental frequencies than females in most cases.

# 5. Jitter Feature

Jitter Feature measures the deviation of periodicity in a periodic signal. It can be computed as:

$$ Jitter(T) = \frac{1}{N-1} \sum_{i=1}^{N-1} \mid T_i - T_{i+1} \mid $$



To continue ...

# Deep Learning

Mention AudioSet and OpenSMILE

# Meta features

In order to add some more features, on can create meta features. These features are the result of a regression or a classification algorithm that is ran halfway through the feature extraction process. We can for example train an algorithm to detect gender based on MFCC features, and for each new sample, predict whether this is a male or a female and add it as a features. 

Among meta features, the most popular are:
- gender
- age category
- ethnicity / accent
- diseases
- emotions
- audio quality
- fatigue level
- stress level

# Dimension reduction

One common issue when dealing with audio features is the number of features created. It can easily exceed 1'000 features on a set of audio samples, and we therefore need to think of dimension reduction techniques. 

The most common unsupervised approaches are:
- Principal Component Analysis (PCA)
- K-means clustering

And for supervised approaches:
- Supervised Dictionary Learning (SDL)
- Linear Discriminant Analysis
- Variational Autoencoders

Most techniques are easy to implement using scikit-learn, but since it's a bit less common, here's how to implement the SDL approach:

```python
# supervised dictionary learning
from sklearn.decomposition import MiniBatchDictionaryLearning
dico_X = MiniBatchDictionaryLearning(n_components=50, alpha=1, n_iter=500).fit_transform(X)
dico_Y = MiniBatchDictionaryLearning(n_components=50, alpha=1, n_iter=500).fit_transform(Y)
```

