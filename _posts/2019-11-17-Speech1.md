---
published: true
title: Speaker Verification using Gaussian Mixture Model (GMM-UBM)
collection: ml
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Speech Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/lgen_head.png"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

The method introduced below is called GMM-UBM, which stands for Gaussian Mixture Model - Universal Background Model. This method has, for a long time, been a state-of-the-art approach.

I will use as a reference the paper: "A Tutorial on Text-Independent Speaker Verification" by Frédétic Bimbot et al. Although from 2002, this tutorial describes this classical approach pretty well.

This article requires that you have understood the [basics of Speaker Verification](https://maelfabien.github.io/machinelearning/basics_speech/).

In this article, I will present the main steps of a Speaker Verification system.

![image](https://maelfabien.github.io/assets/images/bs_1.png)

# I. Speech acquisition and Feature extraction

We should extract features from the signal to convert the raw signal into a sequence of acoustic feature vectures which we will use to identify the speaker. 

Most speech features used in speaker verification rely on a cepstral representation of speech.

## 1. Filterbank-based cepstral parameters

## Pre-emphasis

The first step is usually to apply a pre-emphasis of the signal to enhance the high frequencies of the spectrum, reduced by the speech production process:

$$ x_p(t) = x(t) - a x(t-1) $$ 

Where $$ a $$ takes values between 0.95 and 0.98. 

## Framing

The signal is then split into successive frames. Most of the time, a length of frame of 20 milliseconds is used, with a shift of 10 milliseconds. 

## Windowing

Then, a windowing is applied. Indeed, when you cut your signal into frames, it is most likely that the end of a frame will not match the start of the next frame. Therefore, a windowing function is needed. The Hamming window is one of the most common approaches. Windowing also gives a more accurate idea of the original signal's frequency spectrum, as is "cuts off" signals at their end.

The Hamming window is given by:

$$ w[n]=a_{0}-\underbrace {(1-a_{0})} _{a_{1}}\cdot \cos \left({\tfrac {2\pi n}{N}}\right),\quad 0\leq n\leq N $$

Where $$ a_0 = 0.53836 $$ is the optimal value.

![image](https://maelfabien.github.io/assets/images/hamming.png)

All windowing functions can be found on [Wikipedia](https://en.wikipedia.org/wiki/Window_function).

## Fast Fourier Transform

Then, a FFT algorithm is picked (most often Cooley–Tukey) to compute efficiently the Discrete Fourrier Transform:

$$ X_{k}=\sum _{n=0}^{N-1}x_{n}e^{-i2\pi kn/N}\qquad k=0,\ldots ,N-1 $$ 

We typically make the computation on 512 points.

## Modulus

The absolute value of the FFT is then computed, which gives the magnitude. At that point, we have a *power spectrum* sampled over 512 points. However, since the spectrum is symmetric, only half of those points are useful. 

## Filtering

The spectrum at that point has lots of fluctuations.


A common choice for the features to extract from the signal is the Mel Frequency Cepstral Coefficients (MFCC). These features are derived from Fast Fourier Transform. There are several steps and options for extracting MFCC features, both in time and frequency domains, but again, I'll dive deeper into this topic in another article.

# Voice Activity Detection

During Voice Activity Detection (VAD), we identify which frames to keep and which ones to drop, depending on whether it contains speech or not. A common approach is the Gaussian-based VAD, but one can also use the energy-based VAD. The aim of a VAD is to aquire speech only when it occurs. I described a bit further the concept and implementation of Voice Activity Detection in [this project](https://maelfabien.github.io/project/Speech_proj/#).

The main steps behind building a VAD are:
- Break audio signal into frames
- Extract features from each frame
- Train a classifier on a known set of speech and silence frames
- Classify unseen frames as speech or silence

But this will be the topic of another article. VAD performs well on audio with relatively low signal-to-noice ratio (SNR), a ratio which compares the level of a desired signal to the level of background noise.

### Universal Background Model : Development

The next step, once we extracted the features, is to train a universal background model (UBM). We train such algorithm because there is typically not enough data available to train the speaker models.

A UBM is a high-order Gaussian Mixture Model trained on a large quantity of speech, from a wide population. This step is used to learn speaker-independent distribution of features.

### Speaker Enrolment

The last step before the verification is to perform the speaker enrolement. The aim is still to train a Gaussian Mixture Model on the extracted features. GMMs are typically solved iteratively by an Expectation Maximization (EM) algorithm, an algorithm which tries to maximize the likelihood of the training data by adjusting the parameters of the GMM.

The issue is always to initialize the parameters of the GMM the right way. Since we pre-trained a GMM in the development step, we simply start the EM algorithm with the parameters learned by the UBM.

Through this step, we only adapt the mean, and not the covariance, since updating the covariance does not improve the performance.

For the mean to update, we perform a *maximum a posteriori adaptation* :

$$ \mu_k^{MAP} = \alpha_k \mu_k + (1 - \alpha_k) \mu_k^{UBM} $$

Where :
- $ \alpha_k = \frac{n_k}{n_k + \tau_k} $ is the mean adaptation coefficient
- $ n_k $ is the count for the adaptation data
- $ \tau_k $ is the relevance factor, between 8 and 32

### Speaker Verification

The Speaker Verification is finally computed using the likelihood ratio, since GMM is a generative model.

We test the following underlying hypothesis:
- $$ H_0 $$ : Sample X belongs to claimed speaker s
- $$ H_1 $$ : Sample X does not belong to claimedd speaker s

The likelihood ratio is then defined as :

$$ S(X) = \frac{P(X \mid H_0)} {P(X \mid H_1)} $$

We compare this ratio to a threshold, and if it is greater than the defined threshold, we accept that the sample X belongs to the claimed speaker s.

# Limits of GMM-UBM

Nowadays, GMM-UBM are not state-of-the-art approaches anymore. Indeed, it requires too much training data in general. Better performing approaches have been developed such as :
- SVM-based methods
- I-vector methods
- Deep-learning based methods


